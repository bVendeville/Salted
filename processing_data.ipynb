{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa28621-f588-4b7e-9c5f-f7a217bb481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iaa_df = pd.read_csv(\"data/inter_annotator_agreement.csv\")\n",
    "indiv_df = pd.read_csv(\"data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af899501-a8d1-4875-84ad-6db72f18f22d",
   "metadata": {},
   "source": [
    "## Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796d420f-079b-4bc5-8732-0011a811089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_stats(df):\n",
    "    # Specify columns to exclude\n",
    "    exclude_columns = ['source sentence', 'simplified sentence', 'Comment', 'Commentaire', 'Annotator', 'run_id', 'snt_id']\n",
    "    \n",
    "    # Select only the columns that represent errors (i.e., True/False values)\n",
    "    error_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "    \n",
    "    # Compute statistics for error columns\n",
    "    error_stats = df[error_columns].apply(lambda col: {\n",
    "        'Total Count': len(col),\n",
    "        'True Count': (col == True).sum(),\n",
    "        'False Count': (col == False).sum(),\n",
    "        'Percentage True': ((col == True).sum() / len(col)) * 100\n",
    "    }).to_dict()\n",
    "    \n",
    "    # Convert stats to a DataFrame for better visualization\n",
    "    error_stats_df = pd.DataFrame.from_dict(error_stats, orient='index')\n",
    "\n",
    "    return error_stats_df\n",
    "    \n",
    "    return error_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf30b2e8-263a-411d-9426-c6f72d9eabc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Count</th>\n",
       "      <th>True Count</th>\n",
       "      <th>False Count</th>\n",
       "      <th>Percentage True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <td>610</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>610</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No error</th>\n",
       "      <td>610</td>\n",
       "      <td>263</td>\n",
       "      <td>347</td>\n",
       "      <td>43.114754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1. Random generation</th>\n",
       "      <td>610</td>\n",
       "      <td>39</td>\n",
       "      <td>571</td>\n",
       "      <td>6.393443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2. Syntax error</th>\n",
       "      <td>610</td>\n",
       "      <td>48</td>\n",
       "      <td>562</td>\n",
       "      <td>7.868852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3. Contradiction</th>\n",
       "      <td>610</td>\n",
       "      <td>3</td>\n",
       "      <td>607</td>\n",
       "      <td>0.491803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4. Simple punctuation / grammar errors</th>\n",
       "      <td>610</td>\n",
       "      <td>45</td>\n",
       "      <td>565</td>\n",
       "      <td>7.377049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A5. Redundancy</th>\n",
       "      <td>610</td>\n",
       "      <td>30</td>\n",
       "      <td>580</td>\n",
       "      <td>4.918033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1. Format misalignement</th>\n",
       "      <td>610</td>\n",
       "      <td>24</td>\n",
       "      <td>586</td>\n",
       "      <td>3.934426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2. Prompt misalignement</th>\n",
       "      <td>610</td>\n",
       "      <td>11</td>\n",
       "      <td>599</td>\n",
       "      <td>1.803279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1. Factuality hallucination</th>\n",
       "      <td>610</td>\n",
       "      <td>12</td>\n",
       "      <td>598</td>\n",
       "      <td>1.967213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C2. Faithfulness hallucination</th>\n",
       "      <td>610</td>\n",
       "      <td>42</td>\n",
       "      <td>568</td>\n",
       "      <td>6.885246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C3. Topic shift</th>\n",
       "      <td>610</td>\n",
       "      <td>30</td>\n",
       "      <td>580</td>\n",
       "      <td>4.918033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1.1. Overgeneralization</th>\n",
       "      <td>610</td>\n",
       "      <td>75</td>\n",
       "      <td>535</td>\n",
       "      <td>12.295082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1.2 Overspecification of Concepts</th>\n",
       "      <td>610</td>\n",
       "      <td>27</td>\n",
       "      <td>583</td>\n",
       "      <td>4.426230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2.1. Loss of Informative Content</th>\n",
       "      <td>610</td>\n",
       "      <td>143</td>\n",
       "      <td>467</td>\n",
       "      <td>23.442623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2.2. Out-of-Scope Generation</th>\n",
       "      <td>610</td>\n",
       "      <td>71</td>\n",
       "      <td>539</td>\n",
       "      <td>11.639344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Total Count  True Count  False Count  \\\n",
       "Unnamed: 0.1                                     610           1            1   \n",
       "Unnamed: 0                                       610           1            1   \n",
       "No error                                         610         263          347   \n",
       "A1. Random generation                            610          39          571   \n",
       "A2. Syntax error                                 610          48          562   \n",
       "A3. Contradiction                                610           3          607   \n",
       "A4. Simple punctuation / grammar errors          610          45          565   \n",
       "A5. Redundancy                                   610          30          580   \n",
       "B1. Format misalignement                         610          24          586   \n",
       "B2. Prompt misalignement                         610          11          599   \n",
       "C1. Factuality hallucination                     610          12          598   \n",
       "C2. Faithfulness hallucination                   610          42          568   \n",
       "C3. Topic shift                                  610          30          580   \n",
       "D1.1. Overgeneralization                         610          75          535   \n",
       "D1.2 Overspecification of Concepts               610          27          583   \n",
       "D2.1. Loss of Informative Content                610         143          467   \n",
       "D2.2. Out-of-Scope Generation                    610          71          539   \n",
       "\n",
       "                                         Percentage True  \n",
       "Unnamed: 0.1                                    0.163934  \n",
       "Unnamed: 0                                      0.163934  \n",
       "No error                                       43.114754  \n",
       "A1. Random generation                           6.393443  \n",
       "A2. Syntax error                                7.868852  \n",
       "A3. Contradiction                               0.491803  \n",
       "A4. Simple punctuation / grammar errors         7.377049  \n",
       "A5. Redundancy                                  4.918033  \n",
       "B1. Format misalignement                        3.934426  \n",
       "B2. Prompt misalignement                        1.803279  \n",
       "C1. Factuality hallucination                    1.967213  \n",
       "C2. Faithfulness hallucination                  6.885246  \n",
       "C3. Topic shift                                 4.918033  \n",
       "D1.1. Overgeneralization                       12.295082  \n",
       "D1.2 Overspecification of Concepts              4.426230  \n",
       "D2.1. Loss of Informative Content              23.442623  \n",
       "D2.2. Out-of-Scope Generation                  11.639344  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'number of columns in annotated datasets: 17'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = get_stats(indiv_df)\n",
    "display(stats)\n",
    "display(f\"number of columns in annotated datasets: {stats.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9956ed4-68d7-4356-bd45-e285e7c7a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator counts indiv:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Annotator\n",
       "B    281\n",
       "D     96\n",
       "C     92\n",
       "E     49\n",
       "F     49\n",
       "A     43\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator counts IAA:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Annotator\n",
       "A    104\n",
       "D    104\n",
       "C    104\n",
       "B    104\n",
       "E     52\n",
       "F     23\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the number of rows per annotator for error and no-error cases\n",
    "annotator_counts_indiv = indiv_df[\"Annotator\"].value_counts()\n",
    "# Count the number of rows per annotator for error and no-error cases\n",
    "annotator_counts_iaa = iaa_df[\"Annotator\"].value_counts()\n",
    "\n",
    "# Display results\n",
    "print(\"Annotator counts indiv:\")\n",
    "display(annotator_counts_indiv)\n",
    "\n",
    "# Display results\n",
    "print(\"Annotator counts IAA:\")\n",
    "display(annotator_counts_iaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f9116-eceb-42e9-b1c8-ff097572dcde",
   "metadata": {},
   "source": [
    "# Analyzing Inter Annotators Agreement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "065abccd-358a-40aa-b361-0788c5a5961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "import krippendorff  # pip install krippendorff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15b97e71-fb55-45e0-922c-f6959845a46b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pivot_annotations(df, value_col):\n",
    "    \"\"\"\n",
    "    Pivot the annotation DataFrame so that each row corresponds to a unique item \n",
    "    (using 'item_id', which is a concatenation of snt_id and run_id) and \n",
    "    each column corresponds to an annotator’s rating for the given value_col.\n",
    "    \"\"\"\n",
    "    pivot = df.pivot(index='item_id', columns='Annotator', values=value_col)\n",
    "    return pivot\n",
    "\n",
    "\n",
    "def compute_pairwise_cohen(pivot_df):\n",
    "    \"\"\"\n",
    "    Compute pairwise Cohen's kappa between annotators from the pivot table.\n",
    "    Also compute raw agreement (the fraction of items where the two annotators agree)\n",
    "    and the number of overlapping items.\n",
    "    \n",
    "    If the two annotators’ ratings do not span both possible labels [0, 1],\n",
    "    Cohen's kappa is set to NaN.\n",
    "    \n",
    "    Returns a DataFrame with the following columns:\n",
    "      - Annotator1\n",
    "      - Annotator2\n",
    "      - CohenKappa: the chance-corrected agreement metric.\n",
    "      - RawAgreement: the raw percentage agreement (0–1).\n",
    "      - N_Items: the number of items on which both annotators rated.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    annotators = pivot_df.columns.tolist()\n",
    "    for i in range(len(annotators)):\n",
    "        for j in range(i + 1, len(annotators)):\n",
    "            pair_data = pivot_df[[annotators[i], annotators[j]]].dropna()\n",
    "            union_labels = sorted(set(pair_data[annotators[i]].unique()).union(set(pair_data[annotators[j]].unique())))\n",
    "            if len(union_labels) < 2:\n",
    "                kappa = np.nan\n",
    "            else:\n",
    "                kappa = cohen_kappa_score(pair_data[annotators[i]], pair_data[annotators[j]], labels=[0, 1])\n",
    "            raw = pair_data.apply(lambda row: row[annotators[i]] == row[annotators[j]], axis=1).mean() if len(pair_data) > 0 else np.nan\n",
    "            results.append({\n",
    "                'Annotator1': annotators[i],\n",
    "                'Annotator2': annotators[j],\n",
    "                'CohenKappa': kappa,\n",
    "                'RawAgreement': raw,\n",
    "                'N_Items': len(pair_data)\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def compute_fleiss(pivot_df):\n",
    "    \"\"\"\n",
    "    Compute Fleiss' kappa using only items rated by at least 2 annotators.\n",
    "    For each item, build a count vector over the domain {0, 1}.\n",
    "    If no item qualifies, return NaN.\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "    for idx, row in pivot_df.iterrows():\n",
    "        ratings = row.dropna()\n",
    "        if len(ratings) < 2:\n",
    "            continue  # Need at least 2 ratings.\n",
    "        counts = ratings.value_counts().reindex([0, 1], fill_value=0)\n",
    "        matrix.append(counts.values)\n",
    "    if len(matrix) == 0:\n",
    "        return np.nan\n",
    "    matrix = np.array(matrix)\n",
    "    try:\n",
    "        fk = fleiss_kappa(matrix)\n",
    "        return fk\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def compute_krippendorff(pivot_df):\n",
    "    \"\"\"\n",
    "    Compute Krippendorff's alpha.\n",
    "    The data is transposed so that rows represent raters.\n",
    "    If, after removing missing data, there is only one unique value, return NaN.\n",
    "    \"\"\"\n",
    "    data = pivot_df.to_numpy().T  # shape: (n_raters, n_items)\n",
    "    flattened = data[~np.isnan(data)]\n",
    "    if len(np.unique(flattened)) < 2:\n",
    "        return np.nan\n",
    "    try:\n",
    "        alpha = krippendorff.alpha(reliability_data=data, level_of_measurement='nominal')\n",
    "        return alpha\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def analyze_agreement_for_label(df, value_col, analysis_label):\n",
    "    \"\"\"\n",
    "    For a given column (value_col) in the DataFrame, pivot the data and compute:\n",
    "      - Pairwise Cohen's kappa (per annotator pair), along with raw agreement and N_Items.\n",
    "      - Fleiss' kappa (multi-rater)\n",
    "      - Krippendorff's alpha (multi-rater)\n",
    "    \n",
    "    Returns a dictionary containing:\n",
    "      - 'label': the analysis label (e.g., 'Binary', 'Aggregated_A', or a specific error name)\n",
    "      - 'pivot': the pivoted DataFrame (indexed by item_id)\n",
    "      - 'pairwise': a DataFrame with pairwise metrics (see compute_pairwise_cohen)\n",
    "      - 'fleiss': the Fleiss' kappa value\n",
    "      - 'krippendorff': the Krippendorff's alpha value\n",
    "    \"\"\"\n",
    "    pivot_df = pivot_annotations(df, value_col)\n",
    "    pairwise = compute_pairwise_cohen(pivot_df)\n",
    "    fleiss_val = compute_fleiss(pivot_df)\n",
    "    kripp_val = compute_krippendorff(pivot_df)\n",
    "    return {\n",
    "         'label': analysis_label,\n",
    "         'pivot': pivot_df,\n",
    "         'pairwise': pairwise,\n",
    "         'fleiss': fleiss_val,\n",
    "         'krippendorff': kripp_val\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1abac2e9-1823-4385-9e83-d8c990e335c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_agreement_analysis(df):\n",
    "    \"\"\"\n",
    "    Prepare the DataFrame by:\n",
    "      - Creating an 'item_id' column that combines 'snt_id' and 'run_id'\n",
    "      - Creating a binary classification column: 0 if 'No error' is True, 1 otherwise.\n",
    "      - Creating aggregated error columns for each branch (A, B, C, D)\n",
    "      - Converting specific error columns to integers.\n",
    "      \n",
    "    Then, for each of the following analyses, compute agreement metrics:\n",
    "      - Binary classification\n",
    "      - Aggregated error types (A, B, C, D)\n",
    "      - Each specific error type\n",
    "      \n",
    "    Returns a dictionary with:\n",
    "      - 'pairwise': a DataFrame with pairwise Cohen's kappa (with raw agreement and N_Items) for each analysis category.\n",
    "      - 'multi_rater': a DataFrame with multi-rater metrics (Fleiss' and Krippendorff's alpha) for each category.\n",
    "      - 'detailed': a list of dictionaries (one per analysis category) containing the pivot tables and metrics.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create a unique identifier for each simplification.\n",
    "    df['item_id'] = df['snt_id'].astype(str) + \"_\" + df['run_id'].astype(str)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['item_id', 'Annotator'])\n",
    "    \n",
    "    # --- Prepare Data ---\n",
    "    # Binary classification: 0 if \"No error\" is True, 1 otherwise.\n",
    "    df['binary'] = df['No error'].apply(lambda x: 0 if x else 1)\n",
    "    \n",
    "    # Aggregated error types: for each letter A, B, C, D, mark as 1 if any error in that branch is True.\n",
    "    for letter in ['A', 'B', 'C', 'D']:\n",
    "        error_cols = df.filter(regex=f'^{letter}').columns\n",
    "        df[f'error_{letter}'] = df[error_cols].any(axis=1).astype(int)\n",
    "    \n",
    "    # Specific error types: list of specific error columns.\n",
    "    specific_error_cols = [\n",
    "        'A1. Random generation', 'A2. Syntax error', 'A3. Contradiction',\n",
    "        'A4. Simple punctuation / grammar errors', 'A5. Redundancy',\n",
    "        'B1. Format misalignement', 'B2. Prompt misalignement',\n",
    "        'C1. Factuality hallucination', 'C2. Faithfulness hallucination', 'C3. Topic shift',\n",
    "        'D1.1. Overgeneralization', 'D1.2 Overspecification of Concepts',\n",
    "        'D2.1. Loss of Informative Content', 'D2.2. Out-of-Scope Generation'\n",
    "    ]\n",
    "    for col in specific_error_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(int)\n",
    "    \n",
    "    # --- Compute Agreement ---\n",
    "    analysis_results = []\n",
    "    \n",
    "    # 1. Binary classification\n",
    "    analysis_results.append(analyze_agreement_for_label(df, 'binary', 'Binary'))\n",
    "    \n",
    "    # 2. Aggregated error types (A, B, C, D)\n",
    "    for letter in ['A', 'B', 'C', 'D']:\n",
    "        analysis_results.append(analyze_agreement_for_label(df, f'error_{letter}', f'Aggregated_{letter}'))\n",
    "    \n",
    "    # 3. Specific error types (each individual error)\n",
    "    for col in specific_error_cols:\n",
    "        if col in df.columns:\n",
    "            analysis_results.append(analyze_agreement_for_label(df, col, col))\n",
    "    \n",
    "    # --- Combine Results into DataFrames ---\n",
    "    pairwise_list = []\n",
    "    multi_list = []\n",
    "    \n",
    "    for res in analysis_results:\n",
    "        if res['pairwise'] is not None and not res['pairwise'].empty:\n",
    "            temp = res['pairwise'].copy()\n",
    "            temp['Analysis'] = res['label']\n",
    "            pairwise_list.append(temp)\n",
    "        multi_list.append({\n",
    "            'Analysis': res['label'],\n",
    "            'FleissKappa': res['fleiss'],\n",
    "            'KrippendorffAlpha': res['krippendorff']\n",
    "        })\n",
    "    \n",
    "    df_pairwise = pd.concat(pairwise_list, ignore_index=True) if pairwise_list else pd.DataFrame()\n",
    "    df_multi = pd.DataFrame(multi_list)\n",
    "    \n",
    "    return {\n",
    "        'pairwise': df_pairwise,\n",
    "        'multi_rater': df_multi,\n",
    "        'detailed': analysis_results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68c85b8c-29d1-446a-8fc1-678e66e2e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unanimity(pivot_df):\n",
    "    \"\"\"\n",
    "    For the given pivot table, compute the percentage of items (with at least 2 ratings)\n",
    "    that are unanimously rated (i.e. all non-NaN ratings in the row are equal).\n",
    "    \n",
    "    Returns a percentage (0 to 100) or NaN if no items have at least 2 ratings.\n",
    "    \"\"\"\n",
    "    unanimous = 0\n",
    "    total = 0\n",
    "    for idx, row in pivot_df.iterrows():\n",
    "        ratings = row.dropna()\n",
    "        if len(ratings) >= 2:\n",
    "            total += 1\n",
    "            if len(ratings.unique()) == 1:\n",
    "                unanimous += 1\n",
    "    return (unanimous / total * 100) if total > 0 else np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18217988-d224-4d1d-b6e4-6086cbbcaa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paper_summary(results):\n",
    "    \"\"\"\n",
    "    Given the detailed results from run_agreement_analysis, generate a concise summary DataFrame\n",
    "    suitable for a paper. For each analysis category, the summary includes:\n",
    "      - Analysis: the label\n",
    "      - N_Items: number of unique annotated items\n",
    "      - Mean_Annotators: average number of annotators per item\n",
    "      - FleissKappa: the multi-rater Fleiss' kappa\n",
    "      - KrippendorffAlpha: the multi-rater Krippendorff's alpha\n",
    "      - Percent_Unanimous: percentage of items (with ≥2 ratings) where all annotators agree\n",
    "      - Note: if a metric is NaN, this field indicates if it is likely because annotators fully agree.\n",
    "    \"\"\"\n",
    "    summary_rows = []\n",
    "    for res in results['detailed']:\n",
    "        pivot_df = res['pivot']\n",
    "        n_items = pivot_df.shape[0]\n",
    "        mean_annotators = pivot_df.count(axis=1).mean() if n_items > 0 else np.nan\n",
    "        percent_unanimous = compute_unanimity(pivot_df)\n",
    "        \n",
    "        note = \"\"\n",
    "        # If either multi-rater metric is NaN, check if 100% unanimous items explain this.\n",
    "        if (np.isnan(res['fleiss']) or np.isnan(res['krippendorff'])):\n",
    "            if percent_unanimous == 100:\n",
    "                note = \"NaN due to complete agreement\"\n",
    "            else:\n",
    "                note = \"NaN due to insufficient overlap/variability\"\n",
    "                \n",
    "        summary_rows.append({\n",
    "            \"Analysis\": res['label'],\n",
    "            \"N_Items\": n_items,\n",
    "            \"Mean_Annotators\": mean_annotators,\n",
    "            \"FleissKappa\": res['fleiss'],\n",
    "            \"KrippendorffAlpha\": res['krippendorff'],\n",
    "            \"Percent_Unanimous\": percent_unanimous,\n",
    "            \"Note\": note\n",
    "        })\n",
    "    return pd.DataFrame(summary_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb208cf5-bcc0-4e91-9c19-566e52ffb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairwise_summary_by_category(results):\n",
    "    \"\"\"\n",
    "    Generate a summary table of pairwise agreements for each analysis category.\n",
    "    For each pair (Annotator1, Annotator2) in each category, a summary is created formatted as:\n",
    "       \"CohenKappa/RawAgreement/N_Items\"\n",
    "    Returns a dictionary mapping analysis label to the corresponding summary DataFrame.\n",
    "    \"\"\"\n",
    "    summaries = {}\n",
    "    for res in results['detailed']:\n",
    "        label = res['label']\n",
    "        pairwise_df = res['pairwise']\n",
    "        if pairwise_df is None or pairwise_df.empty:\n",
    "            continue\n",
    "        # Create a summary string column.\n",
    "        def format_summary(row):\n",
    "            ck = row['CohenKappa']\n",
    "            ra = row['RawAgreement']\n",
    "            n = row['N_Items']\n",
    "            ck_str = f\"{ck:.2f}\" if not pd.isna(ck) else \"NaN\"\n",
    "            ra_str = f\"{ra:.2f}\" if not pd.isna(ra) else \"NaN\"\n",
    "            return f\"{ck_str}/{ra_str}/{n}\"\n",
    "        pairwise_df = pairwise_df.copy()\n",
    "        pairwise_df['Summary'] = pairwise_df.apply(format_summary, axis=1)\n",
    "        # Keep only the relevant columns.\n",
    "        summary_df = pairwise_df[['Annotator1', 'Annotator2', 'Summary']]\n",
    "        summaries[label] = summary_df\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3518cc1-b3f9-4936-a756-6a69717da77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_concise_pairwise_summary_table(results, nan_format=\"verbose\", output_cohen_kappa_only=False):\n",
    "    \"\"\"\n",
    "    Generate a single DataFrame where each row corresponds to a unique pair of annotators,\n",
    "    and each additional column is an analysis category. Each cell follows the format:\n",
    "\n",
    "       \"CohenKappa/RawAgreement/N_Items\"\n",
    "       \n",
    "    If `output_cohen_kappa_only` is True, only CohenKappa is output, with NaN formatted according to `nan_format`.\n",
    "    \n",
    "    If Cohen's kappa is NaN, it can be formatted differently based on `nan_format`:\n",
    "      - \"verbose\" (default): \"NaN (reason)\"\n",
    "      - \"short\": \"io\" (insufficient overlap), \"p\" (complete agreement), \"iv\" (insufficient variability)\n",
    "      - None: Leaves NaNs as-is (\"NaN\")\n",
    "    \n",
    "    Parameters:\n",
    "       results (dict): Output of `run_agreement_analysis`\n",
    "       nan_format (str or None): Formatting for NaN values (\"verbose\", \"short\", or None)\n",
    "       output_cohen_kappa_only (bool): If True, only CohenKappa is output.\n",
    "    \n",
    "    Returns:\n",
    "       pd.DataFrame: Annotator pairwise agreement table\n",
    "    \"\"\"\n",
    "    summaries = {}\n",
    "    reason_map = {\n",
    "        \"insufficient overlap\": \"io\",\n",
    "        \"complete agreement\": \"p\",\n",
    "        \"insufficient variability\": \"iv\",\n",
    "    }\n",
    "\n",
    "    for res in results['detailed']:\n",
    "        label = res['label']\n",
    "        pairwise_df = res['pairwise']\n",
    "        if pairwise_df is None or pairwise_df.empty:\n",
    "            continue\n",
    "\n",
    "        def format_summary(row):\n",
    "            ck = row['CohenKappa']\n",
    "            ra = row['RawAgreement']\n",
    "            n = row['N_Items']\n",
    "            \n",
    "            if pd.isna(ck):\n",
    "                if n == 0:\n",
    "                    reason = \"insufficient overlap\"\n",
    "                elif ra == 1.0:\n",
    "                    reason = \"complete agreement\"\n",
    "                else:\n",
    "                    reason = \"insufficient variability\"\n",
    "                \n",
    "                if nan_format == \"verbose\":\n",
    "                    ck_str = f\"NaN ({reason})\"\n",
    "                elif nan_format == \"short\":\n",
    "                    ck_str = reason_map[reason]\n",
    "                else:\n",
    "                    ck_str = \"NaN\"\n",
    "            else:\n",
    "                ck_str = f\"{ck:.2f}\"\n",
    "            \n",
    "            if output_cohen_kappa_only:\n",
    "                return ck_str\n",
    "            \n",
    "            ra_str = f\"{ra:.2f}\" if not pd.isna(ra) else \"NaN\"\n",
    "            return f\"{ck_str}/{ra_str}/{n}\"\n",
    "\n",
    "        df_temp = pairwise_df.copy()\n",
    "        df_temp['Summary'] = df_temp.apply(format_summary, axis=1)\n",
    "        df_temp = df_temp[['Annotator1', 'Annotator2', 'Summary']].rename(columns={'Summary': label})\n",
    "        summaries[label] = df_temp\n",
    "\n",
    "    final_df = None\n",
    "    for label, df_temp in summaries.items():\n",
    "        if final_df is None:\n",
    "            final_df = df_temp.copy()\n",
    "        else:\n",
    "            final_df = pd.merge(final_df, df_temp, on=['Annotator1', 'Annotator2'], how='outer')\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca323f59-343e-47f7-9212-7cca98325115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_annotator_coherence(df, annotation_cols=None):\n",
    "    \"\"\"\n",
    "    Compute the self-consistency rate for each annotator based on duplicate rows.\n",
    "\n",
    "    For each annotator, this function identifies duplicate entries (rows with the same \n",
    "    'snt_id' and 'run_id') and checks if the annotation columns are consistent across duplicates.\n",
    "    The consistency rate is computed as the fraction of duplicate groups with identical annotations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing annotated data. It must have the columns 'Annotator', 'snt_id',\n",
    "        'run_id', and the annotation columns.\n",
    "    annotation_cols : list of str, optional\n",
    "        List of columns that hold the annotation data. If not provided, a default list is used.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with annotators as index and a column 'Consistency Rate' that shows \n",
    "        the self-consistency rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default annotation columns (adjust as needed)\n",
    "    if annotation_cols is None:\n",
    "        annotation_cols = [\n",
    "            'No error',\n",
    "            'A1. Random generation',\n",
    "            'A2. Syntax error',\n",
    "            'A3. Contradiction',\n",
    "            'A4. Simple punctuation / grammar errors',\n",
    "            'A5. Redundancy',\n",
    "            'B1. Format misalignement',\n",
    "            'B2. Prompt misalignement',\n",
    "            'C1. Factuality hallucination',\n",
    "            'C2. Faithfulness hallucination',\n",
    "            'C3. Topic shift',\n",
    "            'D1.1. Overgeneralization',\n",
    "            'D1.2 Overspecification of Concepts',\n",
    "            'D2.1. Loss of Informative Content',\n",
    "            'D2.2. Out-of-Scope Generation'\n",
    "        ]\n",
    "    \n",
    "    # Group the DataFrame by Annotator, snt_id, and run_id.\n",
    "    grouped = df.groupby(['Annotator', 'snt_id', 'run_id'])\n",
    "    \n",
    "    # Dictionary to hold consistency results per annotator\n",
    "    coherence_results = {}\n",
    "    \n",
    "    # For each group, if there are duplicates (group size > 1), check consistency of annotation columns.\n",
    "    for (annotator, snt_id, run_id), group in grouped:\n",
    "        if len(group) > 1:\n",
    "            # Check if each annotation column has a single unique value in the group.\n",
    "            is_consistent = (group[annotation_cols].nunique() == 1).all()\n",
    "            coherence_results.setdefault(annotator, []).append(is_consistent)\n",
    "    \n",
    "    # Compute the consistency rate per annotator.\n",
    "    annotator_coherence = {}\n",
    "    for annotator, results in coherence_results.items():\n",
    "        if results:\n",
    "            consistency_rate = sum(results) / len(results)\n",
    "        else:\n",
    "            consistency_rate = None  # Or you can set to 0 if preferred.\n",
    "        annotator_coherence[annotator] = consistency_rate\n",
    "    \n",
    "    # Create and return a DataFrame with the results.\n",
    "    coherence_df = pd.DataFrame.from_dict(annotator_coherence, orient='index', \n",
    "                                            columns=['Consistency Rate'])\n",
    "    return coherence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65926157-6859-48e8-a57c-f898375353ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keepin only annotators who annotated everything\n",
    "iaa_df = pd.read_csv(\"data/inter_annotator_agreement.csv\")\n",
    "iaa_df = iaa_df[iaa_df['Annotator'].map(iaa_df['Annotator'].value_counts()) == 104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b4d5139-c1e4-4893-8cf2-52c19818bebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consistency Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Consistency Rate\n",
       "A          0.777778\n",
       "B          1.000000\n",
       "C          0.555556\n",
       "D          1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Measuring self-consistency rate for each annotator\n",
    "coherence_df = compute_annotator_coherence(iaa_df)\n",
    "display(coherence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bedccd96-976b-467c-9c11-795164e9b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, suppress warnings during computation:\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    results = run_agreement_analysis(iaa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59a2b8ee-7e8a-4458-97ae-6513d73192d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analysis</th>\n",
       "      <th>N_Items</th>\n",
       "      <th>Mean_Annotators</th>\n",
       "      <th>FleissKappa</th>\n",
       "      <th>KrippendorffAlpha</th>\n",
       "      <th>Percent_Unanimous</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Binary</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.288680</td>\n",
       "      <td>0.290552</td>\n",
       "      <td>41.052632</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aggregated_A</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN due to complete agreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aggregated_B</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.434698</td>\n",
       "      <td>0.436185</td>\n",
       "      <td>80.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aggregated_C</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-0.263333</td>\n",
       "      <td>15.789474</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aggregated_D</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.218107</td>\n",
       "      <td>0.220165</td>\n",
       "      <td>30.526316</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A1. Random generation</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.254227</td>\n",
       "      <td>0.256190</td>\n",
       "      <td>91.578947</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A2. Syntax error</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.024758</td>\n",
       "      <td>0.027325</td>\n",
       "      <td>88.421053</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A3. Contradiction</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.005291</td>\n",
       "      <td>-0.002646</td>\n",
       "      <td>97.894737</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A4. Simple punctuation / grammar errors</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.135536</td>\n",
       "      <td>0.137811</td>\n",
       "      <td>74.736842</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A5. Redundancy</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.010638</td>\n",
       "      <td>-0.007979</td>\n",
       "      <td>95.789474</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B1. Format misalignement</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.011319</td>\n",
       "      <td>80.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B2. Prompt misalignement</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.824074</td>\n",
       "      <td>0.824537</td>\n",
       "      <td>96.842105</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C1. Factuality hallucination</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.041441</td>\n",
       "      <td>0.043964</td>\n",
       "      <td>90.526316</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C2. Faithfulness hallucination</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.109519</td>\n",
       "      <td>0.111863</td>\n",
       "      <td>83.157895</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C3. Topic shift</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.228896</td>\n",
       "      <td>0.230925</td>\n",
       "      <td>71.578947</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D1.1. Overgeneralization</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.236275</td>\n",
       "      <td>0.238284</td>\n",
       "      <td>72.631579</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D1.2 Overspecification of Concepts</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>0.019960</td>\n",
       "      <td>57.894737</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D2.1. Loss of Informative Content</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.418242</td>\n",
       "      <td>0.419773</td>\n",
       "      <td>67.368421</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D2.2. Out-of-Scope Generation</td>\n",
       "      <td>95</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.241153</td>\n",
       "      <td>0.243150</td>\n",
       "      <td>46.315789</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Analysis  N_Items  Mean_Annotators  \\\n",
       "0                                    Binary       95              4.0   \n",
       "1                              Aggregated_A       95              4.0   \n",
       "2                              Aggregated_B       95              4.0   \n",
       "3                              Aggregated_C       95              4.0   \n",
       "4                              Aggregated_D       95              4.0   \n",
       "5                     A1. Random generation       95              4.0   \n",
       "6                          A2. Syntax error       95              4.0   \n",
       "7                         A3. Contradiction       95              4.0   \n",
       "8   A4. Simple punctuation / grammar errors       95              4.0   \n",
       "9                            A5. Redundancy       95              4.0   \n",
       "10                 B1. Format misalignement       95              4.0   \n",
       "11                 B2. Prompt misalignement       95              4.0   \n",
       "12             C1. Factuality hallucination       95              4.0   \n",
       "13           C2. Faithfulness hallucination       95              4.0   \n",
       "14                          C3. Topic shift       95              4.0   \n",
       "15                 D1.1. Overgeneralization       95              4.0   \n",
       "16       D1.2 Overspecification of Concepts       95              4.0   \n",
       "17        D2.1. Loss of Informative Content       95              4.0   \n",
       "18            D2.2. Out-of-Scope Generation       95              4.0   \n",
       "\n",
       "    FleissKappa  KrippendorffAlpha  Percent_Unanimous  \\\n",
       "0      0.288680           0.290552          41.052632   \n",
       "1           NaN                NaN         100.000000   \n",
       "2      0.434698           0.436185          80.000000   \n",
       "3     -0.266667          -0.263333          15.789474   \n",
       "4      0.218107           0.220165          30.526316   \n",
       "5      0.254227           0.256190          91.578947   \n",
       "6      0.024758           0.027325          88.421053   \n",
       "7     -0.005291          -0.002646          97.894737   \n",
       "8      0.135536           0.137811          74.736842   \n",
       "9     -0.010638          -0.007979          95.789474   \n",
       "10     0.008710           0.011319          80.000000   \n",
       "11     0.824074           0.824537          96.842105   \n",
       "12     0.041441           0.043964          90.526316   \n",
       "13     0.109519           0.111863          83.157895   \n",
       "14     0.228896           0.230925          71.578947   \n",
       "15     0.236275           0.238284          72.631579   \n",
       "16     0.017374           0.019960          57.894737   \n",
       "17     0.418242           0.419773          67.368421   \n",
       "18     0.241153           0.243150          46.315789   \n",
       "\n",
       "                             Note  \n",
       "0                                  \n",
       "1   NaN due to complete agreement  \n",
       "2                                  \n",
       "3                                  \n",
       "4                                  \n",
       "5                                  \n",
       "6                                  \n",
       "7                                  \n",
       "8                                  \n",
       "9                                  \n",
       "10                                 \n",
       "11                                 \n",
       "12                                 \n",
       "13                                 \n",
       "14                                 \n",
       "15                                 \n",
       "16                                 \n",
       "17                                 \n",
       "18                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multi-rater (overall) metrics summary (for a paper):\n",
    "df_summary = generate_paper_summary(results)\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49464cf5-dfbc-43ea-874d-cfc7768388fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Annotator1</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annotator2</th>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aggregated_A</th>\n",
       "      <td>p</td>\n",
       "      <td>p</td>\n",
       "      <td>p</td>\n",
       "      <td>p</td>\n",
       "      <td>p</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aggregated_B</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aggregated_C</th>\n",
       "      <td>p</td>\n",
       "      <td>p</td>\n",
       "      <td>0.00</td>\n",
       "      <td>p</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aggregated_D</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1. Random generation</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2. Syntax error</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3. Contradiction</th>\n",
       "      <td>p</td>\n",
       "      <td>0.00</td>\n",
       "      <td>p</td>\n",
       "      <td>0.00</td>\n",
       "      <td>p</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4. Simple punctuation / grammar errors</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A5. Redundancy</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>p</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1. Format misalignement</th>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2. Prompt misalignement</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1. Factuality hallucination</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C2. Faithfulness hallucination</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C3. Topic shift</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1.1. Overgeneralization</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1.2 Overspecification of Concepts</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2.1. Loss of Informative Content</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2.2. Out-of-Scope Generation</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             0      1      2      3      4  \\\n",
       "Annotator1                                   A      A      A      B      B   \n",
       "Annotator2                                   B      C      D      C      D   \n",
       "Binary                                    0.49   0.30   0.16   0.66   0.28   \n",
       "Aggregated_A                                 p      p      p      p      p   \n",
       "Aggregated_B                              0.37   0.58   1.00   0.25   0.37   \n",
       "Aggregated_C                                 p      p   0.00      p   0.00   \n",
       "Aggregated_D                              0.26   0.24   0.14   0.49   0.19   \n",
       "A1. Random generation                    -0.03   0.26   1.00  -0.03  -0.03   \n",
       "A2. Syntax error                         -0.02   0.00  -0.01   0.00   0.17   \n",
       "A3. Contradiction                            p   0.00      p   0.00      p   \n",
       "A4. Simple punctuation / grammar errors   0.29   0.19  -0.02   0.33  -0.02   \n",
       "A5. Redundancy                            0.00   0.00      p  -0.02   0.00   \n",
       "B1. Format misalignement                  0.06  -0.01   0.00   0.09   0.00   \n",
       "B2. Prompt misalignement                  0.90   0.65   1.00   0.74   0.90   \n",
       "C1. Factuality hallucination              0.00   0.00   0.00   0.38  -0.04   \n",
       "C2. Faithfulness hallucination           -0.02   0.18   0.00   0.33   0.00   \n",
       "C3. Topic shift                           0.09   0.27   0.27   0.12   0.55   \n",
       "D1.1. Overgeneralization                  0.11   0.23   0.21   0.29   0.59   \n",
       "D1.2 Overspecification of Concepts        0.07   0.06   0.00   0.18   0.00   \n",
       "D2.1. Loss of Informative Content         0.33   0.35   0.50   0.54   0.46   \n",
       "D2.2. Out-of-Scope Generation             0.27   0.44   0.13   0.23   0.19   \n",
       "\n",
       "                                             5  \n",
       "Annotator1                                   C  \n",
       "Annotator2                                   D  \n",
       "Binary                                    0.23  \n",
       "Aggregated_A                                 p  \n",
       "Aggregated_B                              0.58  \n",
       "Aggregated_C                              0.00  \n",
       "Aggregated_D                              0.20  \n",
       "A1. Random generation                     0.26  \n",
       "A2. Syntax error                          0.00  \n",
       "A3. Contradiction                         0.00  \n",
       "A4. Simple punctuation / grammar errors  -0.02  \n",
       "A5. Redundancy                            0.00  \n",
       "B1. Format misalignement                  0.00  \n",
       "B2. Prompt misalignement                  0.65  \n",
       "C1. Factuality hallucination             -0.03  \n",
       "C2. Faithfulness hallucination            0.00  \n",
       "C3. Topic shift                           0.18  \n",
       "D1.1. Overgeneralization                  0.32  \n",
       "D1.2 Overspecification of Concepts        0.00  \n",
       "D2.1. Loss of Informative Content         0.35  \n",
       "D2.2. Out-of-Scope Generation             0.27  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each cell is formatted as 'CohenKappa/RawAgreement/N_Items', e.g., '0.70/0.95/50'.\n",
      " insufficient overlap = io, complete agreement = p (for perfect), insufficient variability = iv\n"
     ]
    }
   ],
   "source": [
    "# Generate a single DataFrame for all pairwise agreement summaries.\n",
    "concise_pairwise_df = generate_concise_pairwise_summary_table(results, nan_format=\"short\", output_cohen_kappa_only=True)\n",
    "\n",
    "# Display the table.\n",
    "display(concise_pairwise_df.T)\n",
    "\n",
    "# Optionally, print an explanation:\n",
    "print(\"Each cell is formatted as 'CohenKappa/RawAgreement/N_Items', e.g., '0.70/0.95/50'.\")\n",
    "print(\" insufficient overlap = io, complete agreement = p (for perfect), insufficient variability = iv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mirage_venv)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
